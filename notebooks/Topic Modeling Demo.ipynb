{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE</b>: This project is taken from \"Topic Modeling and Latent Dirichlet Allocation (LDA) in Python\" by Susan Li. Check out the blog post in [towardsdatascience.com](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) for better explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data and separate by heading text and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/extracted/abcnews-date-text.csv', error_bad_lines=False)\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103665\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n",
      "                                             headline_text    index\n",
      "1103660  the ashes smiths warners near miss liven up bo...  1103660\n",
      "1103661            timelapse: brisbanes new year fireworks  1103661\n",
      "1103662           what 2017 meant to the kids of australia  1103662\n",
      "1103663   what the papodopoulos meeting may mean for ausus  1103663\n",
      "1103664  who is george papadopoulos the former trump ca...  1103664\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])\n",
    "print(documents[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load gensim and NLTK library; download `wordnet` data through NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/almermendoza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and define functions needed to lemmatize and stem words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check process word output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      " Tokenized and lemmatized document: \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('Original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n Tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak into the output after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out tokens that is not frequent enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bag of words nested-array representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate BOW output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 76 (`bushfir`) appears 1 time.\n",
      "Word 112 (`help`) appears 1 time.\n",
      "Word 483 (`rain`) appears 1 time.\n",
      "Word 4014 (`dampen`) appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (`{}`) appears {} time.\".format(bow_doc_4310[i][0],\n",
    "                                                   dictionary[bow_doc_4310[i][0]],\n",
    "                                                   bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5892908644709983),\n",
      " (1, 0.38929657403503015),\n",
      " (2, 0.4964985198530063),\n",
      " (3, 0.5046520328695662)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LDA model using Bag of Words (and without the TF-IDF model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=10,\n",
    "                                       id2word=dictionary,\n",
    "                                       passes=2,\n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"elect\" + 0.017*\"say\" + 0.016*\"hospit\" + 0.016*\"health\" + 0.015*\"tasmanian\" + 0.014*\"labor\" + 0.014*\"turnbul\" + 0.014*\"report\" + 0.013*\"minist\" + 0.012*\"deal\"\n",
      "Topic: 1 \n",
      "Words: 0.019*\"coast\" + 0.018*\"help\" + 0.018*\"chang\" + 0.017*\"countri\" + 0.016*\"state\" + 0.014*\"hour\" + 0.014*\"indigen\" + 0.013*\"water\" + 0.012*\"gold\" + 0.012*\"communiti\"\n",
      "Topic: 2 \n",
      "Words: 0.019*\"market\" + 0.015*\"rise\" + 0.014*\"price\" + 0.014*\"share\" + 0.013*\"australian\" + 0.012*\"victoria\" + 0.011*\"bank\" + 0.011*\"hous\" + 0.011*\"week\" + 0.010*\"green\"\n",
      "Topic: 3 \n",
      "Words: 0.061*\"polic\" + 0.023*\"crash\" + 0.019*\"interview\" + 0.018*\"miss\" + 0.018*\"shoot\" + 0.016*\"arrest\" + 0.014*\"investig\" + 0.012*\"driver\" + 0.011*\"search\" + 0.011*\"offic\"\n",
      "Topic: 4 \n",
      "Words: 0.029*\"charg\" + 0.027*\"court\" + 0.021*\"murder\" + 0.018*\"woman\" + 0.017*\"face\" + 0.015*\"alleg\" + 0.015*\"brisban\" + 0.015*\"live\" + 0.015*\"jail\" + 0.014*\"perth\"\n",
      "Topic: 5 \n",
      "Words: 0.036*\"australia\" + 0.021*\"world\" + 0.017*\"open\" + 0.016*\"melbourn\" + 0.015*\"sydney\" + 0.014*\"final\" + 0.013*\"donald\" + 0.010*\"leagu\" + 0.010*\"take\" + 0.010*\"win\"\n",
      "Topic: 6 \n",
      "Words: 0.024*\"south\" + 0.023*\"kill\" + 0.021*\"north\" + 0.015*\"west\" + 0.013*\"island\" + 0.012*\"fall\" + 0.011*\"farm\" + 0.010*\"attack\" + 0.009*\"korea\" + 0.009*\"australian\"\n",
      "Topic: 7 \n",
      "Words: 0.016*\"rural\" + 0.014*\"power\" + 0.012*\"farmer\" + 0.011*\"busi\" + 0.011*\"industri\" + 0.011*\"guilti\" + 0.010*\"centr\" + 0.010*\"council\" + 0.009*\"region\" + 0.009*\"liber\"\n",
      "Topic: 8 \n",
      "Words: 0.037*\"trump\" + 0.025*\"australian\" + 0.021*\"canberra\" + 0.020*\"queensland\" + 0.015*\"dead\" + 0.014*\"leav\" + 0.013*\"australia\" + 0.012*\"say\" + 0.010*\"game\" + 0.009*\"meet\"\n",
      "Topic: 9 \n",
      "Words: 0.039*\"govern\" + 0.022*\"test\" + 0.016*\"break\" + 0.014*\"nation\" + 0.013*\"news\" + 0.011*\"violenc\" + 0.010*\"say\" + 0.010*\"worker\" + 0.010*\"premier\" + 0.010*\"hill\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LDA model using the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n",
    "                                             num_topics=10,\n",
    "                                             id2word=dictionary,\n",
    "                                             passes=2,\n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance by classifying sample document using LDA BOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4199999272823334\t \n",
      "Topic: 0.036*\"australia\" + 0.021*\"world\" + 0.017*\"open\" + 0.016*\"melbourn\" + 0.015*\"sydney\" + 0.014*\"final\" + 0.013*\"donald\" + 0.010*\"leagu\" + 0.010*\"take\" + 0.010*\"win\"\n",
      "\n",
      "Score: 0.2200000286102295\t \n",
      "Topic: 0.019*\"coast\" + 0.018*\"help\" + 0.018*\"chang\" + 0.017*\"countri\" + 0.016*\"state\" + 0.014*\"hour\" + 0.014*\"indigen\" + 0.013*\"water\" + 0.012*\"gold\" + 0.012*\"communiti\"\n",
      "\n",
      "Score: 0.21999773383140564\t \n",
      "Topic: 0.024*\"south\" + 0.023*\"kill\" + 0.021*\"north\" + 0.015*\"west\" + 0.013*\"island\" + 0.012*\"fall\" + 0.011*\"farm\" + 0.010*\"attack\" + 0.009*\"korea\" + 0.009*\"australian\"\n",
      "\n",
      "Score: 0.02000226452946663\t \n",
      "Topic: 0.016*\"rural\" + 0.014*\"power\" + 0.012*\"farmer\" + 0.011*\"busi\" + 0.011*\"industri\" + 0.011*\"guilti\" + 0.010*\"centr\" + 0.010*\"council\" + 0.009*\"region\" + 0.009*\"liber\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.027*\"elect\" + 0.017*\"say\" + 0.016*\"hospit\" + 0.016*\"health\" + 0.015*\"tasmanian\" + 0.014*\"labor\" + 0.014*\"turnbul\" + 0.014*\"report\" + 0.013*\"minist\" + 0.012*\"deal\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.019*\"market\" + 0.015*\"rise\" + 0.014*\"price\" + 0.014*\"share\" + 0.013*\"australian\" + 0.012*\"victoria\" + 0.011*\"bank\" + 0.011*\"hous\" + 0.011*\"week\" + 0.010*\"green\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.061*\"polic\" + 0.023*\"crash\" + 0.019*\"interview\" + 0.018*\"miss\" + 0.018*\"shoot\" + 0.016*\"arrest\" + 0.014*\"investig\" + 0.012*\"driver\" + 0.011*\"search\" + 0.011*\"offic\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.029*\"charg\" + 0.027*\"court\" + 0.021*\"murder\" + 0.018*\"woman\" + 0.017*\"face\" + 0.015*\"alleg\" + 0.015*\"brisban\" + 0.015*\"live\" + 0.015*\"jail\" + 0.014*\"perth\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.037*\"trump\" + 0.025*\"australian\" + 0.021*\"canberra\" + 0.020*\"queensland\" + 0.015*\"dead\" + 0.014*\"leav\" + 0.013*\"australia\" + 0.012*\"say\" + 0.010*\"game\" + 0.009*\"meet\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.039*\"govern\" + 0.022*\"test\" + 0.016*\"break\" + 0.014*\"nation\" + 0.013*\"news\" + 0.011*\"violenc\" + 0.010*\"say\" + 0.010*\"worker\" + 0.010*\"premier\" + 0.010*\"hill\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]],\n",
    "                           key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score,\n",
    "                                             lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model on unseen document using LDA BOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3500001132488251\n",
      "Topic: 0.027*\"elect\" + 0.017*\"say\" + 0.016*\"hospit\" + 0.016*\"health\" + 0.015*\"tasmanian\"\n",
      "\n",
      "Score: 0.1833333522081375\n",
      "Topic: 0.019*\"market\" + 0.015*\"rise\" + 0.014*\"price\" + 0.014*\"share\" + 0.013*\"australian\"\n",
      "\n",
      "Score: 0.1833333522081375\n",
      "Topic: 0.061*\"polic\" + 0.023*\"crash\" + 0.019*\"interview\" + 0.018*\"miss\" + 0.018*\"shoot\"\n",
      "\n",
      "Score: 0.18333317339420319\n",
      "Topic: 0.019*\"coast\" + 0.018*\"help\" + 0.018*\"chang\" + 0.017*\"countri\" + 0.016*\"state\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.029*\"charg\" + 0.027*\"court\" + 0.021*\"murder\" + 0.018*\"woman\" + 0.017*\"face\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.036*\"australia\" + 0.021*\"world\" + 0.017*\"open\" + 0.016*\"melbourn\" + 0.015*\"sydney\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.024*\"south\" + 0.023*\"kill\" + 0.021*\"north\" + 0.015*\"west\" + 0.013*\"island\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.016*\"rural\" + 0.014*\"power\" + 0.012*\"farmer\" + 0.011*\"busi\" + 0.011*\"industri\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.037*\"trump\" + 0.025*\"australian\" + 0.021*\"canberra\" + 0.020*\"queensland\" + 0.015*\"dead\"\n",
      "\n",
      "Score: 0.01666666753590107\n",
      "Topic: 0.039*\"govern\" + 0.022*\"test\" + 0.016*\"break\" + 0.014*\"nation\" + 0.013*\"news\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"How a Pentagon deal became an identity crisis for Google\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector],\n",
    "                           key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\nTopic: {}\\n\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_modeling_demo",
   "language": "python",
   "name": "topic_modeling_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
